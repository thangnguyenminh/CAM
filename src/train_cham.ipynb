{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_cham.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"G24BuClCln6t","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","from detector import Detector\n","from util import load_image\n","import os\n","import ipdb\n","\n","weight_path = '/data/Denoise.pickle'\n","model_path = '../models/na/'\n","pretrained_model_path = '' #\"/media/data3/thang/home/PACHA/Weakly_detector/models/Denoise/model-50\"\n","n_epochs = 10\n","init_learning_rate = 0.00001\n","weight_decay_rate = 0.0005\n","momentum = 0.9\n","batch_size = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ltOt-1-lpFq","colab_type":"code","outputId":"579732de-e55b-4abc-b24d-5bbfcf2610af","executionInfo":{"status":"ok","timestamp":1562926265773,"user_tz":-420,"elapsed":26825,"user":{"displayName":"Thắng Nguyễn Minh","photoUrl":"https://lh3.googleusercontent.com/-R1dTC5_z8yQ/AAAAAAAAAAI/AAAAAAAAAFY/DBZz-Bt-Qqg/s64/photo.jpg","userId":"15805147027279298839"}},"colab":{"base_uri":"https://localhost:8080/","height":189}},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ns_M-IAI10xi","colab_type":"code","colab":{}},"source":["sys.version_info"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xqwh1x1Tqmed","colab_type":"code","outputId":"4174d060-69de-4eb5-db20-fe8030a4a8f7","executionInfo":{"status":"ok","timestamp":1562928968448,"user_tz":-420,"elapsed":1595,"user":{"displayName":"Thắng Nguyễn Minh","photoUrl":"https://lh3.googleusercontent.com/-R1dTC5_z8yQ/AAAAAAAAAAI/AAAAAAAAAFY/DBZz-Bt-Qqg/s64/photo.jpg","userId":"15805147027279298839"}},"colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["import cv2\n","img = cv2.imread('/content/drive/My Drive/CAM/inputs/na/Train_set/1.Variant-1/C 24A-line10-cr-letter20.png')\n","img"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       ...,\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]],\n","\n","       [[0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        ...,\n","        [0, 0, 0],\n","        [0, 0, 0],\n","        [0, 0, 0]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"W2Llx7uMrL-5","colab_type":"code","colab":{}},"source":["import zipfile\n","import io\n","data = zipfile.ZipFile('na.zip', 'r')\n","data.extractall()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtZ0W4qfrEZr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOFo70Jeln6z","colab_type":"code","colab":{}},"source":["dataset_path = '../inputs/na'\n","#dataset_path = '../inputs/Denoise'\n","\n","cham_train_path = '../data/na/Train_set'\n","cham_test_path = '../data/na/Test_set'\n","trainset_path = '../data/na/train.pickle'\n","testset_path = '../data/na/test.pickle'\n","label_dict_path = '../data/na/label_dict.pickle'\n","\n","extensions = set(['.png','.jpg'])\n","\n","if not os.path.exists( trainset_path ):\n","    if not os.path.exists( cham_train_path ):\n","        os.makedirs( cham_train_path )\n","        os.makedirs( cham_test_path)\n","    train_dir_list = os.listdir( dataset_path + '/Train_set' )\n","    test_dir_list = os.listdir( dataset_path + '/Test_set' )\n","    label_pairs = map(lambda x: x.split('.'), train_dir_list)\n","    labels, label_names = zip(*label_pairs)\n","    labels = map(lambda x: int(x), labels)\n","    label_dict = pd.Series( labels, index=label_names )\n","    label_dict -= 1 # starting from 0\n","    n_labels = len( label_dict )\n","\n","    train_paths_per_label = list(map(lambda one_dir: map(lambda one_file: os.path.join( dataset_path + '/Train_set', one_dir, one_file ), os.listdir( os.path.join( dataset_path + '/Train_set', one_dir))), train_dir_list))\n","    test_paths_per_label = list(map(lambda one_dir: map(lambda one_file: os.path.join( dataset_path + '/Test_set', one_dir, one_file ), os.listdir( os.path.join( dataset_path + '/Test_set', one_dir))), test_dir_list))\n","\n","    image_paths_train = np.hstack(train_paths_per_label)\n","    image_paths_test = np.hstack(test_paths_per_label)\n","    \n","    trainset = pd.DataFrame({'image_path': image_paths_train})\n","    testset  = pd.DataFrame({'image_path': image_paths_test })\n","    \n","    trainset = trainset[ trainset['image_path'].map( lambda x: x.endswith('.png'))]\n","    trainset['label'] = trainset['image_path'].map(lambda x: int(x.split('/')[-2].split('.')[0])-1)\n","    trainset['label_name'] = trainset['image_path'].map(lambda x: x.split('/')[-2].split('.')[1])\n","\n","    testset = testset[ testset['image_path'].map( lambda x: str(x).endswith('.png'))]\n","    testset['label'] = testset['image_path'].map(lambda x: int(x.split('/')[-2].split('.')[0])-1)\n","    testset['label_name'] = testset['image_path'].map(lambda x: x.split('/')[-2].split('.')[1])\n","\n","    label_dict.to_pickle(label_dict_path)\n","    trainset.to_pickle(trainset_path)\n","    testset.to_pickle(testset_path)\n","\n","else:\n","    trainset = pd.read_pickle( trainset_path )\n","    testset  = pd.read_pickle( testset_path )\n","    label_dict = pd.read_pickle( label_dict_path )\n","    n_labels = len(label_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um9_2yiEln63","colab_type":"code","colab":{}},"source":["# https://stackoverflow.com/a/46760323/9815299\n","trainset['image_path'] = [train_path.replace('/media/data3/thang/home/PACHA/Weakly_detector/inputs/na/Resized/', '../inputs/na/') for train_path in trainset['image_path']]\n","testset['image_path'] = [testset.replace('/media/data3/thang/home/PACHA/Weakly_detector/inputs/na/Resized/', '../inputs/na/') for testset in testset['image_path']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrmNn3FJln67","colab_type":"code","outputId":"7755b465-a451-40a3-c0d5-e43917a3f326","colab":{}},"source":["trainset.image_path[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/Users/mac/Documents/workspace/Python/PACH-code/CAM/inputs/na/Train_set/4.Variant-4/C 38 D-or-line11-cr-letter1.png'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"mNllKUusln6-","colab_type":"code","outputId":"c8c83bd5-6c7d-421f-dcde-55051725342c","colab":{}},"source":["# Training\n","# Train loss = nan: https://stackoverflow.com/a/40434284/9815299\n","learning_rate = tf.placeholder( tf.float32, [])\n","images_tf = tf.placeholder( tf.float32, [None, 224, 224, 3], name=\"images\")\n","labels_tf = tf.placeholder( tf.int64, [None], name='laels')\n","\n","detector = Detector(weight_path, n_labels)\n","\n","p1,p2,p3,p4,conv5, conv6, gap, output = detector.inference(images_tf)\n","loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits( logits=output, labels=labels_tf ) + 1e-10 )\n","\n","weights_only = filter( lambda x: x.name.endswith('W:0'), tf.trainable_variables() )\n","weight_decay = tf.reduce_sum(tf.stack([tf.nn.l2_loss(x) for x in weights_only])) * weight_decay_rate\n","loss_tf += weight_decay\n","\n","# Assume that you have 12GB of GPU memory and want to allocate ~4GB:\n","gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n","\n","sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n","saver = tf.train.Saver( max_to_keep=100 )\n","\n","optimizer = tf.train.MomentumOptimizer( learning_rate, momentum )\n","grads_and_vars = optimizer.compute_gradients( loss_tf )\n","grads_and_vars = map(lambda gv: (gv[0], gv[1]) if ('conv6' in gv[1].name or 'GAP' in gv[1].name) else (gv[0]*0.1, gv[1]), grads_and_vars)\n","#grads_and_vars = [(tf.clip_by_value(gv[0], -5., 5.), gv[1]) for gv in grads_and_vars]\n","train_op = optimizer.apply_gradients( grads_and_vars )\n","tf.global_variables_initializer().run()\n","\n","if pretrained_model_path:\n","    print (\"Pretrained\")\n","    saver.restore(sess, pretrained_model_path)\n","\n","testset.index  = range( len(testset) )\n","#testset = testset.ix[np.random.permutation( len(testset) )]#[:1000]\n","#trainset2 = testset[1000:]\n","#testset = testset[:1000]\n","\n","#trainset = pd.concat( [trainset, trainset2] )\n","# We lack the number of training set. Let's use some of the test images\n","\n","f_log = open('../results/log.Denoise.txt', 'w')\n","\n","iterations = 0\n","loss_list = []\n","for epoch in range(n_epochs):\n","\n","    trainset.index = range( len(trainset) )\n","    trainset = trainset.ix[ np.random.permutation( len(trainset) )]\n","\n","    for start, end in zip(\n","        range( 0, len(trainset)+batch_size, batch_size),\n","        range(batch_size, len(trainset)+batch_size, batch_size)):\n","\n","        current_data = trainset[start:end]\n","        current_image_paths = current_data['image_path'].values\n","        current_images = np.array(map(lambda x: load_image(x), current_image_paths))\n","        \n","        good_index = np.array(map(lambda x: x is not None, current_images))\n","        #good_index = np.fromiter(map(lambda x: x is not None, current_images), dtype=float)\n","        current_data = current_data[good_index]\n","        current_images = np.stack(current_images[good_index])\n","        current_labels = current_data['label'].values\n","\n","        _, loss_val, output_val = sess.run(\n","                [train_op, loss_tf, output],\n","                feed_dict={\n","                    learning_rate: init_learning_rate,\n","                    images_tf: current_images,\n","                    labels_tf: current_labels\n","                    })\n","\n","        loss_list.append( loss_val )\n","\n","        iterations += 1\n","        if iterations % 5 == 0:\n","            print (\"======================================\")\n","            print (\"Epoch\", epoch, \"Iteration\", iterations)\n","            print (\"Processed\", start, '/', len(trainset))\n","\n","            label_predictions = output_val.argmax(axis=1)\n","            acc = (label_predictions == current_labels).sum()\n","\n","            print (\"Accuracy:\", acc, '/', len(current_labels))\n","            print (\"Training Loss:\", np.mean(loss_list))\n","            print (\"\\n\")\n","            loss_list = []\n","\n","    n_correct = 0\n","    n_data = 0\n","    for start, end in zip(\n","            range(0, len(testset)+batch_size, batch_size),\n","            range(batch_size, len(testset)+batch_size, batch_size)\n","            ):\n","        current_data = testset[start:end]\n","        current_image_paths = current_data['image_path'].values\n","        current_images = np.array(map(lambda x: load_image(x), current_image_paths))\n","\n","        good_index = np.array(map(lambda x: x is not None, current_images))\n","\n","        current_data = current_data[good_index]\n","        current_images = np.stack(current_images[good_index])\n","        current_labels = current_data['label'].values\n","\n","        output_vals = sess.run(\n","                output,\n","                feed_dict={images_tf:current_images})\n","\n","        label_predictions = output_vals.argmax(axis=1)\n","        acc = (label_predictions == current_labels).sum()\n","\n","        n_correct += acc\n","        n_data += len(current_data)\n","\n","    acc_all = n_correct / float(n_data)\n","    f_log.write('epoch:'+str(epoch)+'\\tacc:'+str(acc_all) + '\\n')\n","    print (\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n","    print ('epoch:'+str(epochoch)+'\\tacc:'+str(acc_all) + '\\n')\n","    print (\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n","\n","    saver.save( sess, os.path.join( model_path, 'model'), global_step=epoch)\n","\n","    init_learning_rate *= 0.99"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: \n",".ix is deprecated. Please use\n",".loc for label based indexing or\n",".iloc for positional indexing\n","\n","See the documentation here:\n","http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"],"name":"stderr"},{"output_type":"error","ename":"TypeError","evalue":"iteration over a 0-d array","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-af3db6671b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcurrent_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_image_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mgood_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m#good_index = np.fromiter(map(lambda x: x is not None, current_images), dtype=float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcurrent_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgood_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"]}]},{"cell_type":"markdown","metadata":{"id":"-8ReeK_lln7B","colab_type":"text"},"source":["# save weights: https://github.com/google/prettytensor/issues/6#issuecomment-380919368\n","\n","vars = tf.trainable_variables()\n","print(vars) #some infos about variables...\n","vars_vals = sess.run(vars)\n","\n","new_weights = {}\n","for var, val in zip(vars, vars_vals):\n","    #print(\"var: {}, value: {}\".format(var.name, val)) #...or sort it in a list....\n","    layer_name = str(var.name.split('/')[0])\n","    \n","    if layer_name in new_weights:\n","        # append the new number to the existing array at this slot\n","        new_weights[layer_name].append(val)\n","    else:\n","        # create a new array in this slot\n","        new_weights[layer_name] = [val]\n","new_weights"]},{"cell_type":"markdown","metadata":{"id":"W66EAGWkln7B","colab_type":"text"},"source":["#https://stackoverflow.com/a/11218504/9815299\n","with open('../data/na_weights.pickle', 'wb') as handle:\n","    pickle.dump(new_weights, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}]}